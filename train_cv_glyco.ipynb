{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.metrics import matthews_corrcoef as mcc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_val_predict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.lightning_glyco import ImpGroupedBatchSampler\n",
    "from src.models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glyco_data(glyco_type: str):\n",
    "    glyco_to_class = {\n",
    "        'N': 1,\n",
    "        'O': 2\n",
    "        }\n",
    "    glyco_class = glyco_type\n",
    "    glyco_sites = pd.read_csv(f'data/glyco/{glyco_type}/{glyco_type}_train_RR.csv')\n",
    "    val_glyco_sites = pd.read_csv(f'data/glyco/{glyco_type}/{glyco_type}_val_RR.csv')\n",
    "    glyco_sites['split'] = 'train'\n",
    "    val_glyco_sites['split'] = 'train'\n",
    "    \n",
    "    glyco_sites = pd.concat([glyco_sites, val_glyco_sites])\n",
    "    #glyco_sites = glyco_sites[(glyco_sites['label'] == glyco_to_class[glyco_class]) | (glyco_sites['label'] == 0)]\n",
    "\n",
    "    #glyco_sites[\"label\"] = glyco_sites[\"label\"].apply(lambda x: 1 if x >= 1 else 0)\n",
    "    if glyco_class == 'N':\n",
    "        glyco_sites = glyco_sites[glyco_sites[\"AA\"] == 'N']\n",
    "    else:\n",
    "        glyco_sites = glyco_sites[(glyco_sites[\"AA\"] == 'S') | (glyco_sites[\"AA\"] == 'T')]\n",
    "    #glyco_sites, labels = ros.fit_resample(glyco_sites, labels)\n",
    "    glyco_sites.reset_index(drop=True, inplace=True)\n",
    "    input_features = np.empty((len(glyco_sites), 2304))\n",
    "    for idx, (pid, pos) in tqdm(enumerate(zip(glyco_sites['PID'], glyco_sites['Position']))):\n",
    "        input_feature = np.empty(2304)\n",
    "        with h5py.File(f'data/glyco/glyco_embeddings.h5', 'r') as p5, h5py.File(f'data/glyco/glyco_esm_embeddings.h5', 'r') as esm:\n",
    "            try:\n",
    "                #processed_pids = [pid.replace(\"-\", \"_\").replace(\".\", \"_\") for pid in pids] \n",
    "                input_feature = np.concatenate([p5[pid.replace('-', '_').replace('.','_')][()][pos - 1], esm[pid.replace(\"_\", \"-\")][()][pos - 1]])\n",
    "            except:\n",
    "                continue\n",
    "            input_features[idx] = input_feature\n",
    "    mask = np.all(input_features != 0, axis=1)\n",
    "    input_features = input_features[mask]\n",
    "    labels = np.array(glyco_sites['label'])[mask]\n",
    "    print(np.sum(~mask))\n",
    "    print(glyco_sites['label'][mask].value_counts())\n",
    "    print(glyco_sites['label'].value_counts())\n",
    "    input_features = input_features.astype(np.float32)\n",
    "    labels = labels.astype(np.float16)\n",
    "    return input_features, labels, glyco_sites[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glyco_data_3class():\n",
    "    glyco_sites = pd.read_csv(f'data/glyco/combined/train_RR.csv')\n",
    "    glyco_sites['split'] = 'train'\n",
    "    glyco_sites.loc[(glyco_sites['label'] == 0) & (glyco_sites['AA'] != 'N'), 'label'] = 3\n",
    "    glyco_sites.reset_index(drop=True, inplace=True)\n",
    "    input_features = np.empty((len(glyco_sites), 2304))\n",
    "    for idx, (pid, pos) in tqdm(enumerate(zip(glyco_sites['PID'], glyco_sites['Position']))):\n",
    "        input_feature = np.empty(2304)\n",
    "        with h5py.File(f'data/glyco/glyco_embeddings.h5', 'r') as p5, h5py.File(f'data/glyco/glyco_esm_embeddings.h5', 'r') as esm:\n",
    "            try:\n",
    "                #processed_pids = [pid.replace(\"-\", \"_\").replace(\".\", \"_\") for pid in pids] \n",
    "                input_feature = np.concatenate([p5[pid.replace('-', '_').replace('.','_')][()][pos - 1], esm[pid.replace(\"_\", \"-\")][()][pos - 1]])\n",
    "            except:\n",
    "                continue\n",
    "            input_features[idx] = input_feature\n",
    "    mask = np.all(input_features != 0, axis=1)\n",
    "    input_features = input_features[mask]\n",
    "    labels = np.array(glyco_sites['label'])[mask]\n",
    "    print(np.sum(~mask))\n",
    "    print(glyco_sites['label'][mask].value_counts())\n",
    "    print(glyco_sites['label'].value_counts())\n",
    "    input_features = input_features.astype(np.float32)\n",
    "    labels = labels.astype(np.float16)\n",
    "    return input_features, labels, glyco_sites[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sasa_or_bfactor: str):\n",
    "    if sasa_or_bfactor == 'sasa':\n",
    "        data_type = 'sasa'\n",
    "    else:\n",
    "        data_type = 'bfactor'\n",
    "    train = pd.read_csv(f'data/e_prsa/{data_type}/train.csv')\n",
    "    input_features = []\n",
    "    ys = []\n",
    "    pids = np.array(train['PID'].unique())\n",
    "    for idx, pid in tqdm(enumerate(train['PID'].unique())):\n",
    "        train_protein = train[train['PID'] == pid]\n",
    "        y = train_protein['label'].values[0].astype(np.float32)\n",
    "        input_feature = None\n",
    "        with h5py.File(f'data/e_prsa/prott5_sasa_bfactor.h5', 'r') as p5, h5py.File(f'data/e_prsa/esm_sasa_bfactor.h5', 'r') as esm:\n",
    "            try:\n",
    "                #processed_pids = [pid.replace(\"-\", \"_\").replace(\".\", \"_\") for pid in pids] \n",
    "                input_feature = np.concatenate([p5[pid.replace('-', '_').replace('.','_')][()], esm[pid.replace(\"_\", \"-\")][()]])\n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "            ys.append(y)\n",
    "            input_features.append(input_feature)\n",
    "    input_features = np.array(input_features, dtype=object)\n",
    "    ys = np.array(ys, dtype=object)\n",
    "    return input_features, ys, pids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glycodataset(Dataset):\n",
    "    def __init__(self, X, y, pids):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.pids = pids\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.pids[idx]\n",
    "\n",
    "class CVDataset(Dataset):\n",
    "    def __init__(self, input_features, ys):\n",
    "        self.input_features = input_features\n",
    "        self.ys = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_features[idx], self.ys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_glyco_no_oversamp(glyco_class: str, \n",
    "                   input_features, \n",
    "                   labels, \n",
    "                   glyco_sites, \n",
    "                   undersample,\n",
    "                   batch_size,\n",
    "                   hidden_size,\n",
    "                   folds=5):\n",
    "    train_idx = list(glyco_sites[glyco_sites['split'] == 'train'].index)\n",
    "    pids = np.array(glyco_sites['PID'].values)\n",
    "    \n",
    "    train_X_o, train_y_o, train_pids_o = input_features[train_idx], labels[train_idx], pids[train_idx]\n",
    "    if undersample:\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        train_idx_o, train_y_o = rus.fit_resample(np.arange(len(train_y_o)).reshape((-1, 1)), train_y_o)\n",
    "        train_idx_o = train_idx_o.squeeze()\n",
    "        train_X_o = train_X_o[train_idx_o]\n",
    "        train_pids_o = train_pids_o[train_idx_o]\n",
    "    \n",
    "    cv_metrics = {}\n",
    "    \n",
    "    sfold = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=11442)\n",
    "    for idx, (train_idx, val_idx) in enumerate(sfold.split(train_X_o, train_y_o, groups=train_pids_o)):\n",
    "        train_X, train_y, train_pids = train_X_o[train_idx], train_y_o[train_idx], train_pids_o[train_idx]\n",
    "        val_X, val_y, val_pids = train_X_o[val_idx], train_y_o[val_idx], train_pids_o[val_idx]\n",
    "\n",
    "        \n",
    "        train_dataset = Glycodataset(train_X, train_y, train_pids)\n",
    "        val_dataset = Glycodataset(val_X, val_y, val_pids)\n",
    "        train_dl = DataLoader(train_dataset, batch_sampler=ImpGroupedBatchSampler(train_pids, batch_size=batch_size))\n",
    "        val_dl = DataLoader(val_dataset, 1, shuffle=False)\n",
    "        glyco_model = GlycoModel(num_classes=2, \n",
    "                                 lr=0.0001,\n",
    "                                 input_dim=2304, \n",
    "                                 num_hidden=hidden_size, \n",
    "                                 num_layers=2,\n",
    "                                 class_weights=torch.tensor(train_y[train_y == 0].shape[0] / train_y[train_y == 1].shape[0]).to('cuda'))\n",
    "        checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)\n",
    "        tensor_b = TensorBoardLogger('tb_logs', \n",
    "                                     name=f'glyco_{glyco_class}_cv_{idx}', \n",
    "                                     default_hp_metric=False)\n",
    "        #tensor_b = WandbLogger(name=f'glyco_{glyco_class}_cv_{idx}', project='protein_properties_cv')\n",
    "        trainer = Trainer(max_epochs=30, \n",
    "                          enable_progress_bar=False, \n",
    "                          num_sanity_val_steps=1, \n",
    "                          logger=tensor_b,\n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=4), checkpoint_callback])        \n",
    "        trainer.fit(glyco_model, train_dl, val_dl)\n",
    "        \n",
    "        y_pred_val = trainer.predict(ckpt_path='best', \n",
    "                                     dataloaders=DataLoader(val_dataset, 1, shuffle=False))\n",
    "        val_loss = np.array([loss[1] for loss in y_pred_val]).mean()\n",
    "        y_pred_val = np.array([pred[0] for pred in y_pred_val], dtype=np.float16)\n",
    "        matt_val = mcc(val_y, y_pred_val)\n",
    "        f1_val = f1(val_y, y_pred_val)\n",
    "        acc_val = np.mean(val_y == y_pred_val)\n",
    "        #loss_val = cross_entropy(torch.tensor(y_pred_val), torch.tensor(val_y).long())\n",
    "        \n",
    "        y_pred_train = trainer.predict(ckpt_path='best', \n",
    "                                       dataloaders=DataLoader(train_dataset, batch_size=1, shuffle=False))\n",
    "        train_loss = np.array([loss[1] for loss in y_pred_train]).mean()\n",
    "        y_pred_train = np.array([pred[0] for pred in y_pred_train], dtype=np.float16)\n",
    "        matt_train = mcc(train_y, y_pred_train)\n",
    "        f1_train = f1(train_y, y_pred_train)\n",
    "        acc_train = np.mean(train_y == y_pred_train)\n",
    "        #loss_train = cross_entropy(torch.tensor(y_pred_train), torch.tensor(train_y).long())\n",
    "        \n",
    "        cv_metrics[f'fold_{idx}'] = {\n",
    "            'matt_train': matt_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_train': acc_train,\n",
    "            'loss_train': train_loss,\n",
    "            'matt_val': matt_val,\n",
    "            'f1_val': f1_val,\n",
    "            'acc_val': acc_val,\n",
    "            'loss_val': val_loss,\n",
    "            'model': checkpoint_callback.best_model_path,\n",
    "            'train_pred': y_pred_train,\n",
    "            'val_pred': y_pred_val,\n",
    "            'train_true': train_y,\n",
    "            'val_true': val_y\n",
    "        }\n",
    "        \n",
    "    return cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_glyco(glyco_class: str, \n",
    "                   input_features, \n",
    "                   labels, \n",
    "                   glyco_sites, \n",
    "                   us_ratio, \n",
    "                   os_ratio, \n",
    "                   batch_size,\n",
    "                   hidden_size,\n",
    "                   folds=7):\n",
    "    train_idx = list(glyco_sites[glyco_sites['split'] == 'train'].index)\n",
    "    pids = np.array(glyco_sites['PID'].values)\n",
    "    \n",
    "    train_X_o, train_y_o, train_pids_o = input_features[train_idx], labels[train_idx], pids[train_idx]\n",
    "    \n",
    "    \n",
    "    cv_metrics = {}\n",
    "    \n",
    "    sfold = StratifiedGroupKFold(n_splits=folds, shuffle=True, random_state=13442)\n",
    "    for idx, (train_idx, val_idx) in enumerate(sfold.split(train_X_o, train_y_o, groups=train_pids_o)):\n",
    "        train_X, train_y, train_pids = train_X_o[train_idx], train_y_o[train_idx], train_pids_o[train_idx]\n",
    "        val_X, val_y, val_pids = train_X_o[val_idx], train_y_o[val_idx], train_pids_o[val_idx]\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        rus_r = RandomUnderSampler(sampling_strategy={0: int(train_y[train_y == 1].shape[0] * us_ratio), \n",
    "                                                    1: train_y[train_y == 1].shape[0]}, random_state=42)\n",
    "        train_idx, train_y = rus_r.fit_resample(np.arange(len(train_y)).reshape((-1, 1)), train_y)\n",
    "        train_idx = train_idx.squeeze()\n",
    "        train_X = train_X[train_idx]\n",
    "        train_pids = train_pids[train_idx]\n",
    "        \n",
    "        ros = RandomOverSampler(sampling_strategy={0: train_y[train_y == 0].shape[0], \n",
    "                                                    1: int(train_y[train_y == 1].shape[0] * os_ratio)}, random_state=42)\n",
    "        train_idx, train_y = ros.fit_resample(np.arange(len(train_y)).reshape((-1, 1)), train_y)\n",
    "        train_idx = train_idx.squeeze()\n",
    "        train_X = train_X[train_idx]\n",
    "        train_pids = train_pids[train_idx]\n",
    "        \n",
    "        val_idx, val_y = rus.fit_resample(np.arange(len(val_y)).reshape((-1, 1)), val_y)\n",
    "        val_idx = val_idx.squeeze()\n",
    "        val_X = val_X[val_idx]\n",
    "        val_pids = val_pids[val_idx]\n",
    "        \n",
    "        train_dataset = Glycodataset(train_X, train_y, train_pids)\n",
    "        val_dataset = Glycodataset(val_X, val_y, val_pids)\n",
    "        train_dl = DataLoader(train_dataset, batch_sampler=ImpGroupedBatchSampler(train_pids, batch_size=batch_size))\n",
    "        val_dl = DataLoader(val_dataset, 1, shuffle=False)\n",
    "        glyco_model = GlycoModel(num_classes=2, \n",
    "                                 lr=0.0001,\n",
    "                                 input_dim=2304, \n",
    "                                 num_hidden=hidden_size, \n",
    "                                 num_layers=2,\n",
    "                                 class_weights=torch.tensor(train_y[train_y == 0].shape[0] / train_y[train_y == 1].shape[0]).to('cuda'))\n",
    "        checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)\n",
    "        tensor_b = TensorBoardLogger('tb_logs', \n",
    "                                     name=f'glyco_{glyco_class}_cv_{idx}', \n",
    "                                     default_hp_metric=False)\n",
    "        #tensor_b = WandbLogger(name=f'glyco_{glyco_class}_cv_{idx}', project='protein_properties_cv')\n",
    "        trainer = Trainer(max_epochs=30, \n",
    "                          enable_progress_bar=False, \n",
    "                          num_sanity_val_steps=1, \n",
    "                          logger=tensor_b,\n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=4), checkpoint_callback])        \n",
    "        trainer.fit(glyco_model, train_dl, val_dl)\n",
    "        \n",
    "        y_pred_val = trainer.predict(ckpt_path='best', \n",
    "                                     dataloaders=DataLoader(val_dataset, 1, shuffle=False))\n",
    "        val_loss = np.array([loss[1] for loss in y_pred_val]).mean()\n",
    "        y_pred_val = np.array([pred[0] for pred in y_pred_val], dtype=np.float16)\n",
    "        matt_val = mcc(val_y, y_pred_val)\n",
    "        f1_val = f1(val_y, y_pred_val)\n",
    "        acc_val = np.mean(val_y == y_pred_val)\n",
    "        #loss_val = cross_entropy(torch.tensor(y_pred_val), torch.tensor(val_y).long())\n",
    "        \n",
    "        y_pred_train = trainer.predict(ckpt_path='best', \n",
    "                                       dataloaders=DataLoader(train_dataset, batch_size=1, shuffle=False))\n",
    "        train_loss = np.array([loss[1] for loss in y_pred_train]).mean()\n",
    "        y_pred_train = np.array([pred[0] for pred in y_pred_train], dtype=np.float16)\n",
    "        matt_train = mcc(train_y, y_pred_train)\n",
    "        f1_train = f1(train_y, y_pred_train)\n",
    "        acc_train = np.mean(train_y == y_pred_train)\n",
    "        #loss_train = cross_entropy(torch.tensor(y_pred_train), torch.tensor(train_y).long())\n",
    "        \n",
    "        cv_metrics[f'fold_{idx}'] = {\n",
    "            'matt_train': matt_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_train': acc_train,\n",
    "            'loss_train': train_loss,\n",
    "            'matt_val': matt_val,\n",
    "            'f1_val': f1_val,\n",
    "            'acc_val': acc_val,\n",
    "            'loss_val': val_loss,\n",
    "            'model': checkpoint_callback.best_model_path,\n",
    "            'train_pred': y_pred_train,\n",
    "            'val_pred': y_pred_val,\n",
    "            'train_true': train_y,\n",
    "            'val_true': val_y\n",
    "        }\n",
    "        \n",
    "    return cv_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_glyco_3class(\n",
    "                   input_features, \n",
    "                   labels, \n",
    "                   glyco_sites, \n",
    "                   undersample,\n",
    "                   batch_size,\n",
    "                   hidden_size,\n",
    "                   folds=5):\n",
    "    train_idx = list(glyco_sites[glyco_sites['split'] == 'train'].index)\n",
    "    pids = np.array(glyco_sites['PID'].values)\n",
    "    \n",
    "    train_X_o, train_y_o, train_pids_o = input_features[train_idx], labels[train_idx], pids[train_idx]\n",
    "    if undersample:\n",
    "        rus = RandomUnderSampler(random_state=42, sampling_strategy={0: train_y_o[train_y_o == 0].shape[0], \n",
    "                                                                    1: train_y_o[train_y_o == 1].shape[0],\n",
    "                                                                    2: train_y_o[train_y_o == 2].shape[0],\n",
    "                                                                    3: train_y_o[train_y_o == 2].shape[0]})\n",
    "        train_idx_o, train_y_o = rus.fit_resample(np.arange(len(train_y_o)).reshape((-1, 1)), train_y_o)\n",
    "        train_idx_o = train_idx_o.squeeze()\n",
    "        train_X_o = train_X_o[train_idx_o]\n",
    "        train_pids_o = train_pids_o[train_idx_o]\n",
    "    \n",
    "    cv_metrics = {}\n",
    "    \n",
    "    train_y_o = torch.tensor(train_y_o).long()\n",
    "    sfold = StratifiedGroupKFold(n_splits=folds, shuffle=True, random_state=11442)\n",
    "    for idx, (train_idx, val_idx) in enumerate(sfold.split(train_X_o, train_y_o, groups=train_pids_o)):\n",
    "        train_X, train_y, train_pids = train_X_o[train_idx], train_y_o[train_idx], train_pids_o[train_idx]\n",
    "        val_X, val_y, val_pids = train_X_o[val_idx], train_y_o[val_idx], train_pids_o[val_idx]\n",
    "        train_y[train_y == 3] = 0\n",
    "        val_y[val_y == 3] = 0\n",
    "        \n",
    "        train_dataset = Glycodataset(train_X, train_y, train_pids)\n",
    "        val_dataset = Glycodataset(val_X, val_y, val_pids)\n",
    "        train_dl = DataLoader(train_dataset, batch_sampler=ImpGroupedBatchSampler(train_pids, batch_size=batch_size))\n",
    "        val_dl = DataLoader(val_dataset, 1, shuffle=False)\n",
    "        class_weights = torch.tensor(compute_class_weight(class_weight='balanced', classes=np.unique(np.array(train_y)), y=np.array(train_y))).float().cuda()\n",
    "        glyco_model = GlycoModel(num_classes=3, \n",
    "                                 lr=0.0001,\n",
    "                                 input_dim=2304, \n",
    "                                 num_hidden=hidden_size, \n",
    "                                 num_layers=2,\n",
    "                                 class_weights=class_weights\n",
    "                                 )\n",
    "        checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)\n",
    "        tensor_b = TensorBoardLogger('tb_logs', \n",
    "                                     name=f'glyco_3class_cv_{idx}', \n",
    "                                     default_hp_metric=False)\n",
    "        #tensor_b = WandbLogger(name=f'glyco_{glyco_class}_cv_{idx}', project='protein_properties_cv')\n",
    "        trainer = Trainer(max_epochs=30, \n",
    "                          enable_progress_bar=False, \n",
    "                          num_sanity_val_steps=1, \n",
    "                          logger=tensor_b,\n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=4), checkpoint_callback])        \n",
    "        trainer.fit(glyco_model, train_dl, val_dl)\n",
    "        \n",
    "        y_pred_val = trainer.predict(ckpt_path='best', \n",
    "                                     dataloaders=DataLoader(val_dataset, 1, shuffle=False))\n",
    "        val_loss = np.array([loss[1] for loss in y_pred_val]).mean()\n",
    "        y_pred_val = np.array([pred[0] for pred in y_pred_val], dtype=np.float16)\n",
    "        matt_val = mcc(val_y, y_pred_val)\n",
    "        f1_val = f1(val_y, y_pred_val, average='micro')\n",
    "        acc_val = np.mean(val_y == y_pred_val)\n",
    "        #loss_val = cross_entropy(torch.tensor(y_pred_val), torch.tensor(val_y).long())\n",
    "        \n",
    "        y_pred_train = trainer.predict(ckpt_path='best', \n",
    "                                       dataloaders=DataLoader(train_dataset, batch_size=1, shuffle=False))\n",
    "        train_loss = np.array([loss[1] for loss in y_pred_train]).mean()\n",
    "        y_pred_train = np.array([pred[0] for pred in y_pred_train], dtype=np.float16)\n",
    "        matt_train = mcc(train_y, y_pred_train)\n",
    "        f1_train = f1(train_y, y_pred_train, average='micro')\n",
    "        acc_train = np.mean(train_y == y_pred_train)\n",
    "        #loss_train = cross_entropy(torch.tensor(y_pred_train), torch.tensor(train_y).long())\n",
    "        \n",
    "        cv_metrics[f'fold_{idx}'] = {\n",
    "            'matt_train': matt_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_train': acc_train,\n",
    "            'loss_train': train_loss,\n",
    "            'matt_val': matt_val,\n",
    "            'f1_val': f1_val,\n",
    "            'acc_val': acc_val,\n",
    "            'loss_val': val_loss,\n",
    "            'model': checkpoint_callback.best_model_path,\n",
    "            'train_pred': y_pred_train,\n",
    "            'val_pred': y_pred_val,\n",
    "            'train_true': train_y,\n",
    "            'val_true': val_y\n",
    "        }\n",
    "        \n",
    "    return cv_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N glyco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15292it [02:15, 112.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n",
      "label\n",
      "0    8815\n",
      "1    5807\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    9228\n",
      "1    6064\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_features, labels, glyco_sites  = prepare_glyco_data('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "2024-11-26 15:18:41.277102: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 15:18:41.285015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-26 15:18:41.299563: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-26 15:18:41.304025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 15:18:41.315215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-26 15:18:42.499084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('val_MCC', ...)` in your `on_validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('train_MCC', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 36. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_0/version_5/checkpoints/epoch=29-step=5460.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_0/version_5/checkpoints/epoch=29-step=5460.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_0/version_5/checkpoints/epoch=29-step=5460.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_0/version_5/checkpoints/epoch=29-step=5460.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_1/version_3/checkpoints/epoch=28-step=5278.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_1/version_3/checkpoints/epoch=28-step=5278.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_1/version_3/checkpoints/epoch=28-step=5278.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_1/version_3/checkpoints/epoch=28-step=5278.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_2/version_2/checkpoints/epoch=29-step=5550.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_2/version_2/checkpoints/epoch=29-step=5550.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_N_cv_2/version_2/checkpoints/epoch=29-step=5550.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_N_cv_2/version_2/checkpoints/epoch=29-step=5550.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv_metrics_n_nosampling \u001b[38;5;241m=\u001b[39m \u001b[43mcv_train_glyco_no_oversamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglyco_sites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m84\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m, in \u001b[0;36mcv_train_glyco_no_oversamp\u001b[0;34m(glyco_class, input_features, labels, glyco_sites, undersample, batch_size, hidden_size, folds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#loss_val = cross_entropy(torch.tensor(y_pred_val), torch.tensor(val_y).long())\u001b[39;00m\n\u001b[1;32m     59\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     60\u001b[0m                                dataloaders\u001b[38;5;241m=\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m---> 61\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([loss[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m y_pred_train])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     62\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m y_pred_train], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     63\u001b[0m matt_train \u001b[38;5;241m=\u001b[39m mcc(train_y, y_pred_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "cv_metrics_n_nosampling = cv_train_glyco_no_oversamp('N', input_features, labels, glyco_sites, False, 64, [84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics = cv_train_glyco('N', input_features, labels, glyco_sites, 0.8, 1.5, 128, [84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to disk\n",
    "import pickle   \n",
    "with open('glyco_N_cv_nosamp_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_metrics_n_nosampling, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O glyco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88282it [03:51, 381.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8636\n",
      "label\n",
      "0    75727\n",
      "1     3919\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    84087\n",
      "1     4195\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_features, labels, glyco_sites  = prepare_glyco_data('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics_o_nosampling = cv_train_glyco_no_oversamp('O', input_features, labels, glyco_sites, True, 32, [46], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics_o = cv_train_glyco('O', input_features, labels, glyco_sites, 1.0, 1.7, 16, [46], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glyco_O_cv_nosamp_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_metrics_o_nosampling, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 class Glyco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105497it [05:57, 294.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396\n",
      "label\n",
      "3    79243\n",
      "0    11284\n",
      "1     6906\n",
      "2     3668\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "3    82624\n",
      "0    11819\n",
      "1     7211\n",
      "2     3843\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_features, labels, glyco_sites = prepare_glyco_data_3class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 45. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_0/version_18/checkpoints/epoch=29-step=9570.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_0/version_18/checkpoints/epoch=29-step=9570.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_0/version_18/checkpoints/epoch=29-step=9570.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_0/version_18/checkpoints/epoch=29-step=9570.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/glyco_3class_cv_1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_1/version_0/checkpoints/epoch=28-step=9222.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_1/version_0/checkpoints/epoch=28-step=9222.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_1/version_0/checkpoints/epoch=28-step=9222.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_1/version_0/checkpoints/epoch=28-step=9222.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/glyco_3class_cv_2\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 41. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_2/version_0/checkpoints/epoch=29-step=9600.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_2/version_0/checkpoints/epoch=29-step=9600.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_2/version_0/checkpoints/epoch=29-step=9600.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_2/version_0/checkpoints/epoch=29-step=9600.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/glyco_3class_cv_3\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_3/version_0/checkpoints/epoch=28-step=9251.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_3/version_0/checkpoints/epoch=28-step=9251.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_3/version_0/checkpoints/epoch=28-step=9251.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_3/version_0/checkpoints/epoch=28-step=9251.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/glyco_3class_cv_4\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 193 K \n",
      "1 | softmax | Softmax    | 0     \n",
      "2 | sigmoid | Sigmoid    | 0     \n",
      "---------------------------------------\n",
      "193 K     Trainable params\n",
      "0         Non-trainable params\n",
      "193 K     Total params\n",
      "0.775     Total estimated model params size (MB)\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 59. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_4/version_0/checkpoints/epoch=29-step=9630.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_4/version_0/checkpoints/epoch=29-step=9630.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/glyco_3class_cv_4/version_0/checkpoints/epoch=29-step=9630.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/glyco_3class_cv_4/version_0/checkpoints/epoch=29-step=9630.ckpt\n",
      "/home/d/PycharmProjects/protein_properties/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "cv_metric_3class = cv_train_glyco_3class(input_features, labels, glyco_sites, True, 64, [84], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_0': {'matt_train': 0.7468104236460702,\n",
       "  'f1_train': 0.8360543217139775,\n",
       "  'acc_train': 0.0,\n",
       "  'loss_train': 0.7187252880522051,\n",
       "  'matt_val': 0.6185779479153861,\n",
       "  'f1_val': 0.7555078962760772,\n",
       "  'acc_val': 0.0,\n",
       "  'loss_val': 0.7896809572131342,\n",
       "  'model': 'tb_logs/glyco_3class_cv_0/version_18/checkpoints/epoch=29-step=9570.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 0., 2., 0.], dtype=float16),\n",
       "  'val_pred': array([0., 1., 0., ..., 2., 0., 0.], dtype=float16),\n",
       "  'train_true': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "  'val_true': tensor([0, 0, 0,  ..., 0, 0, 0])},\n",
       " 'fold_1': {'matt_train': 0.7362472274954647,\n",
       "  'f1_train': 0.825373427672956,\n",
       "  'acc_train': 0.0,\n",
       "  'loss_train': 0.7310795588038111,\n",
       "  'matt_val': 0.6544667484302495,\n",
       "  'f1_val': 0.7709702357943564,\n",
       "  'acc_val': 0.0,\n",
       "  'loss_val': 0.7734390697656196,\n",
       "  'model': 'tb_logs/glyco_3class_cv_1/version_0/checkpoints/epoch=28-step=9222.ckpt',\n",
       "  'train_pred': array([0., 0., 1., ..., 0., 2., 0.], dtype=float16),\n",
       "  'val_pred': array([1., 0., 0., ..., 2., 0., 0.], dtype=float16),\n",
       "  'train_true': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "  'val_true': tensor([0, 0, 0,  ..., 0, 0, 0])},\n",
       " 'fold_2': {'matt_train': 0.7439244105195382,\n",
       "  'f1_train': 0.8310602727672679,\n",
       "  'acc_train': 0.0,\n",
       "  'loss_train': 0.7239799516440055,\n",
       "  'matt_val': 0.6105052146901017,\n",
       "  'f1_val': 0.755770368909055,\n",
       "  'acc_val': 0.0,\n",
       "  'loss_val': 0.7885882429974064,\n",
       "  'model': 'tb_logs/glyco_3class_cv_2/version_0/checkpoints/epoch=29-step=9600.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 0., 0., 0.], dtype=float16),\n",
       "  'val_pred': array([0., 0., 0., ..., 2., 0., 2.], dtype=float16),\n",
       "  'train_true': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "  'val_true': tensor([0, 0, 0,  ..., 0, 0, 0])},\n",
       " 'fold_3': {'matt_train': 0.7382989859974528,\n",
       "  'f1_train': 0.8245002210324672,\n",
       "  'acc_train': 0.0,\n",
       "  'loss_train': 0.7307905765271643,\n",
       "  'matt_val': 0.617649128716843,\n",
       "  'f1_val': 0.7557576930520612,\n",
       "  'acc_val': 0.0,\n",
       "  'loss_val': 0.7891208632254891,\n",
       "  'model': 'tb_logs/glyco_3class_cv_3/version_0/checkpoints/epoch=28-step=9251.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 0., 2., 0.], dtype=float16),\n",
       "  'val_pred': array([1., 1., 1., ..., 0., 2., 0.], dtype=float16),\n",
       "  'train_true': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "  'val_true': tensor([0, 0, 0,  ..., 0, 0, 0])},\n",
       " 'fold_4': {'matt_train': 0.7394676094525025,\n",
       "  'f1_train': 0.8261843322459711,\n",
       "  'acc_train': 0.0,\n",
       "  'loss_train': 0.7282187793879251,\n",
       "  'matt_val': 0.6293196612467562,\n",
       "  'f1_val': 0.7585722879486665,\n",
       "  'acc_val': 0.0,\n",
       "  'loss_val': 0.784877167834245,\n",
       "  'model': 'tb_logs/glyco_3class_cv_4/version_0/checkpoints/epoch=29-step=9630.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 2., 0., 2.], dtype=float16),\n",
       "  'val_pred': array([0., 1., 0., ..., 0., 0., 2.], dtype=float16),\n",
       "  'train_true': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       "  'val_true': tensor([0, 0, 0,  ..., 0, 0, 0])}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_metric_3class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glyco_3class_cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_metric_3class, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate CV metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glyco_N_cv_results.pkl', 'rb') as f:\n",
    "    cv_metrics_N = pickle.load(f)\n",
    "with open('glyco_O_cv_results.pkl', 'rb') as f:\n",
    "    cv_metrics_O = pickle.load(f)\n",
    "with open('glyco_N_cv_nosamp_results.pkl', 'rb') as f:\n",
    "    cv_metrics_N_nosamp = pickle.load(f)\n",
    "with open('glyco_O_cv_nosamp_results.pkl', 'rb') as f:\n",
    "    cv_metrics_O_nosamp = pickle.load(f)\n",
    "with open('glyco_3class_cv_results.pkl', 'rb') as f:\n",
    "    cv_metric_3class = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_0': {'matt_train': 0.7160192522171801,\n",
       "  'f1_train': 0.8493195074530137,\n",
       "  'acc_train': 0.8556348959950326,\n",
       "  'loss_train': 0.5656767462914274,\n",
       "  'matt_val': 0.6129905200359977,\n",
       "  'f1_val': 0.7670068027210885,\n",
       "  'acc_val': 0.8037249283667621,\n",
       "  'loss_val': 0.5978884983882522,\n",
       "  'model': 'tb_logs/glyco_O_cv_0/version_1/checkpoints/epoch=26-step=5454.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_pred': array([0., 0., 0., ..., 0., 1., 1.], dtype=float16),\n",
       "  'train_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16)},\n",
       " 'fold_1': {'matt_train': 0.735558639539423,\n",
       "  'f1_train': 0.8591668072187553,\n",
       "  'acc_train': 0.8662502002242511,\n",
       "  'loss_train': 0.572299803905374,\n",
       "  'matt_val': 0.593143400514179,\n",
       "  'f1_val': 0.7831245880026367,\n",
       "  'acc_val': 0.793730407523511,\n",
       "  'loss_val': 0.5983842329545455,\n",
       "  'model': 'tb_logs/glyco_O_cv_1/version_1/checkpoints/epoch=28-step=5684.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_pred': array([0., 0., 0., ..., 0., 0., 1.], dtype=float16),\n",
       "  'train_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16)},\n",
       " 'fold_2': {'matt_train': 0.7376231254716806,\n",
       "  'f1_train': 0.8596858638743455,\n",
       "  'acc_train': 0.867828374157488,\n",
       "  'loss_train': 0.5791376437792825,\n",
       "  'matt_val': 0.5024683610128671,\n",
       "  'f1_val': 0.716841455891425,\n",
       "  'acc_val': 0.7384615384615385,\n",
       "  'loss_val': 0.6131375478543447,\n",
       "  'model': 'tb_logs/glyco_O_cv_2/version_1/checkpoints/epoch=27-step=5348.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_pred': array([0., 0., 0., ..., 1., 1., 0.], dtype=float16),\n",
       "  'train_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16)},\n",
       " 'fold_3': {'matt_train': 0.7223782908163502,\n",
       "  'f1_train': 0.8521626856036152,\n",
       "  'acc_train': 0.8590118516238264,\n",
       "  'loss_train': 0.5671282629242342,\n",
       "  'matt_val': 0.6220176739626552,\n",
       "  'f1_val': 0.7901234567901234,\n",
       "  'acc_val': 0.8098434004474273,\n",
       "  'loss_val': 0.597358220020041,\n",
       "  'model': 'tb_logs/glyco_O_cv_3/version_1/checkpoints/epoch=29-step=6120.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_pred': array([0., 0., 0., ..., 1., 0., 1.], dtype=float16),\n",
       "  'train_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16)},\n",
       " 'fold_4': {'matt_train': 0.7167445365807396,\n",
       "  'f1_train': 0.849551414768806,\n",
       "  'acc_train': 0.8567438804008543,\n",
       "  'loss_train': 0.5740359353180959,\n",
       "  'matt_val': 0.6373885059228133,\n",
       "  'f1_val': 0.8144690781796966,\n",
       "  'acc_val': 0.8183894917190178,\n",
       "  'loss_val': 0.5915035158480868,\n",
       "  'model': 'tb_logs/glyco_O_cv_4/version_1/checkpoints/epoch=27-step=5348.ckpt',\n",
       "  'train_pred': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_pred': array([1., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'train_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16),\n",
       "  'val_true': array([0., 0., 0., ..., 1., 1., 1.], dtype=float16)}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_metrics_O_nosamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cv_metrics(cv_metrics_N, title):\n",
    "    # dicts to arrays\n",
    "    matt_val_N = np.array([cv_metrics_N[f'fold_{i}']['matt_val'] for i in range(5)])\n",
    "    f1_val_N = np.array([cv_metrics_N[f'fold_{i}']['f1_val'] for i in range(5)])\n",
    "    acc_val_N = np.array([cv_metrics_N[f'fold_{i}']['acc_val'] for i in range(5)])\n",
    "    loss_val_N = np.array([cv_metrics_N[f'fold_{i}']['loss_val'] for i in range(5)])\n",
    "    matt_train_N = np.array([cv_metrics_N[f'fold_{i}']['matt_train'] for i in range(5)])\n",
    "    f1_train_N = np.array([cv_metrics_N[f'fold_{i}']['f1_train'] for i in range(5)])\n",
    "    acc_train_N = np.array([cv_metrics_N[f'fold_{i}']['acc_train'] for i in range(5)])\n",
    "    loss_train_N = np.array([cv_metrics_N[f'fold_{i}']['loss_train'] for i in range(5)])\n",
    "    # display results\n",
    "    print(f'{title}')\n",
    "    print('Validation')\n",
    "    print(f'MCC: {mean_confidence_interval(matt_val_N)}')\n",
    "    print(f'F1: {mean_confidence_interval(f1_val_N)}')\n",
    "    print(f'Accuracy: {mean_confidence_interval(acc_val_N)}')\n",
    "    print(f'Loss: {mean_confidence_interval(loss_val_N)}')\n",
    "    print('Training')\n",
    "    print(f'MCC: {mean_confidence_interval(matt_train_N)}')\n",
    "    print(f'F1: {mean_confidence_interval(f1_train_N)}')\n",
    "    print(f'Accuracy: {mean_confidence_interval(acc_train_N)}')\n",
    "    print(f'Loss: {mean_confidence_interval(loss_train_N)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Glycosylation with resampling\n",
      "Validation\n",
      "MCC: (0.48723930199084303, 0.03325045947600155)\n",
      "F1: (0.7269751001751958, 0.01795409639741133)\n",
      "Accuracy: (0.7419858227182345, 0.0164121145917134)\n",
      "Loss: (0.5256116059249847, 0.007316343052187019)\n",
      "Training\n",
      "MCC: (0.6001168182009299, 0.0164731138961925)\n",
      "F1: (0.8298044726863629, 0.014753099569902648)\n",
      "Accuracy: (0.7971249746143145, 0.013567001106316949)\n",
      "Loss: (0.41664553938649596, 0.0015515123913180083)\n"
     ]
    }
   ],
   "source": [
    "calculate_cv_metrics(cv_metrics_N, 'N Glycosylation with resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Glycosylation without resampling\n",
      "Validation\n",
      "MCC: (0.4946204404846449, 0.03053684150374917)\n",
      "F1: (0.6982733902361147, 0.01780999910349806)\n",
      "Accuracy: (0.7565729284402753, 0.015485499720937693)\n",
      "Loss: (0.7501042101337616, 0.00783195992301314)\n",
      "Training\n",
      "MCC: (0.6492416766538454, 0.01087751331179394)\n",
      "F1: (0.7896217750018247, 0.007966082128577554)\n",
      "Accuracy: (0.8313197923153611, 0.005179743300540183)\n",
      "Loss: (0.715584298354519, 0.003264926927211513)\n"
     ]
    }
   ],
   "source": [
    "calculate_cv_metrics(cv_metrics_N_nosamp, 'N Glycosylation without resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Glycosylation with resampling\n",
      "Validation\n",
      "MCC: (0.55529108898251, 0.047172140978392514)\n",
      "F1: (0.7529535652643737, 0.018501649912094843)\n",
      "Accuracy: (0.7732222918133042, 0.02119787519557048)\n",
      "Loss: (0.5177197247839391, 0.014986831916968405)\n",
      "Training\n",
      "MCC: (0.6946359157654367, 0.07330761598211553)\n",
      "F1: (0.8661113654773402, 0.04267501466033582)\n",
      "Accuracy: (0.8442043961398153, 0.04377269814898921)\n",
      "Loss: (0.42413555231828737, 0.010886699785268199)\n"
     ]
    }
   ],
   "source": [
    "calculate_cv_metrics(cv_metrics_O, 'O Glycosylation with resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Glycosylation without resampling\n",
      "Validation\n",
      "MCC: (0.5936016922897025, 0.06630466510668313)\n",
      "F1: (0.7743130763169941, 0.04518225804070069)\n",
      "Accuracy: (0.7928299533036512, 0.039354007240398406)\n",
      "Loss: (0.5996544030130541, 0.009976605335670066)\n",
      "Training\n",
      "MCC: (0.7256647689250747, 0.012788211561977469)\n",
      "F1: (0.8539772557837072, 0.006334160090041874)\n",
      "Accuracy: (0.8610938404802905, 0.006941082078840988)\n",
      "Loss: (0.5716556784436829, 0.0067526845637055615)\n"
     ]
    }
   ],
   "source": [
    "calculate_cv_metrics(cv_metrics_O_nosamp, 'O Glycosylation without resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 class Glycosylation\n",
      "Validation\n",
      "MCC: (0.6261037401998674, 0.02138254270338311)\n",
      "F1: (0.7593156963960432, 0.008238817732687692)\n",
      "Accuracy: (0.0, 0.0)\n",
      "Loss: (0.785141260207179, 0.008451956430241537)\n",
      "Training\n",
      "MCC: (0.7409497314222058, 0.005359801578878789)\n",
      "F1: (0.8286345150865279, 0.006040548758827899)\n",
      "Accuracy: (0.0, 0.0)\n",
      "Loss: (0.7265588308830223, 0.0064850974439327105)\n"
     ]
    }
   ],
   "source": [
    "calculate_cv_metrics(cv_metric_3class, '3 class Glycosylation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot cv metric for all 4 cross validation runs compare O against O and N against N\n",
    "# use the mean confidence interval for the error bars\n",
    "def plot_cv_nosamp_vs_samp(cv_metrics, cv_metrics_nosamp):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "    for i, (glyco_class, title) in enumerate(zip(['N', 'O'], ['N Glycosylation', 'O Glycosylation'])):\n",
    "        # dicts to arrays\n",
    "        matt_val = np.array([cv_metrics[f'fold_{i}']['matt_val'] for i in range(5)])\n",
    "        f1_val = np.array([cv_metrics[f'fold_{i}']['f1_val'] for i in range(5)])\n",
    "        acc_val = np.array([cv_metrics[f'fold_{i}']['acc_val'] for i in range(5)])\n",
    "        loss_val = np.array([cv_metrics[f'fold_{i}']['loss_val'] for i in range(5)])\n",
    "        matt_train = np.array([cv_metrics[f'fold_{i}']['matt_train'] for i in range(5)])\n",
    "        f1_train = np.array([cv_metrics[f'fold_{i}']['f1_train'] for i in range(5)])\n",
    "        acc_train = np.array([cv_metrics[f'fold_{i}']['acc_train'] for i in range(5)])\n",
    "        loss_train = np.array([cv_metrics[f'fold_{i}']['loss_train'] for i in range(5)])\n",
    "        # display results\n",
    "        print(f'{title}')\n",
    "        print('Validation')\n",
    "        print(f'MCC: {mean_confidence_interval(matt_val)}')\n",
    "        print(f'F1: {mean_confidence_interval(f1_val)}')\n",
    "        print(f'Accuracy: {mean_confidence_interval(acc_val)}')\n",
    "        print(f'Loss: {mean_confidence_interval(loss_val)}')\n",
    "        print('Training')\n",
    "        print(f'MCC: {mean_confidence_interval(matt_train)}')\n",
    "        print(f'F1: {mean_confidence_interval(f1_train)}')\n",
    "        print(f'Accuracy: {mean_confidence_interval(acc_train)}')\n",
    "        print(f'Loss: {mean_confidence_interval(loss_train)}')\n",
    "        # dicts to arrays\n",
    "        matt_val_nosamp = np.array([cv_metrics_nosamp[f'fold_{i}']['matt_val'] for i in range(5)])\n",
    "        f1_val_nosamp = np.array([cv_metrics_nosamp[f'fold_{i}']['f1_val'] for i in range(5)])\n",
    "        acc_val_nosamp = np.array([cv_metrics_nosamp[f'fold_{i}']['acc_val'] for i in range(5)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
